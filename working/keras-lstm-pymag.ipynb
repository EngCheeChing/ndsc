{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21f85dfd5b9bf3d8b27ba29149d52253e5d64049"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"../input/ndscloaded/x_train.npy\")\n",
    "x_test = np.load(\"../input/ndscloaded/x_test.npy\")\n",
    "y_train = np.load(\"../input/ndscloaded/y_train.npy\")\n",
    "features = np.load(\"../input/ndscloaded/features.npy\")\n",
    "test_features = np.load(\"../input/ndscloaded/test_features.npy\")\n",
    "word_index = np.load(\"../input/ndscloaded/word_index.npy\").item()\n",
    "embedding_matrix = np.load(\"../input/ndscloaded/embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(embedding_matrix[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "indices = np.arange(len(x_train)-1)\n",
    "np.random.shuffle(indices)\n",
    "i = 0\n",
    "x_train_batch_tokens = np.take(x_train, indices[i*batch_size:(i+1)*batch_size], axis = 0) \n",
    "x_train_batch = [[embedding_matrix[token] for token in title] for title in x_train_batch_tokens]\n",
    "y_train_batch_targets = np.take(y_train, indices[i*batch_size:(i+1)*batch_size])\n",
    "y_train_batch = np.zeros((batch_size, 58))\n",
    "y_train_batch[np.arange(batch_size), y_train_batch_targets] = 1\n",
    "\n",
    "print(indices)\n",
    "print(x_train_batch_tokens)\n",
    "print(np.shape(x_train_batch[:1]))\n",
    "print(y_train_batch_targets)\n",
    "print(y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0c950448e0717eaebb920d93cfdc6b4561e21853"
   },
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 128\n",
    "\n",
    "indices = np.arange(len(x_train)-1)\n",
    "# print(len(x_train))\n",
    "# print(np.random.shuffle())\n",
    "\n",
    "def batch_gen():\n",
    "    n_batches = math.ceil(len(x_train) / batch_size)\n",
    "    while True: \n",
    "        np.random.shuffle(indices)  # Shuffle the data.\n",
    "#         print(indices)\n",
    "        for i in range(n_batches):\n",
    "#             texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "#             text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            \n",
    "            x_train_batch_tokens = np.take(x_train, indices[i*batch_size:(i+1)*batch_size], axis = 0) \n",
    "            x_train_batch = [[embedding_matrix[token] for token in title] for title in x_train_batch_tokens]\n",
    "            y_train_batch_targets = np.take(y_train, indices[i*batch_size:(i+1)*batch_size])\n",
    "            y_train_batch = np.zeros((batch_size, 58))\n",
    "            y_train_batch[np.arange(batch_size), y_train_batch_targets] = 1\n",
    "\n",
    "            yield (x_train_batch, y_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "706c0224b6112e5a8f00ad25f35e90fbb9519a5f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "798c303ec834fb530a60a1e590cfbd9a86f93fde"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9ad418c95b31ab5691d49b72c7c9622ef9ea42cf"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n",
    "                        input_shape=(30, 300)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "model.add(Dense(58))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3f0b703ded691293450beb4ddf2d903d0e08c737",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a2c8ab75bdd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(mg, epochs=1,\n\u001b[1;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     verbose=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mg = batch_gen()\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6cdda0e301be0b84e826e29f0f3a85c41aa6da9"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b58cd95254f41e5002a17de0c3feab54a5fc3c67"
   },
   "outputs": [],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"../input/ndsc-beginner/test.csv\")\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e6ed54def110c881f401c6f8a844752baedcfbd"
   },
   "outputs": [],
   "source": [
    "print(np.shape(all_preds)\n",
    "y_te = [np.argmax(pred) for pred in all_preds]\n",
    "\n",
    "submit_df = pd.DataFrame({\"itemid\": test_df[\"itemid\"], \"Category\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
