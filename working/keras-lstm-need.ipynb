{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21f85dfd5b9bf3d8b27ba29149d52253e5d64049"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"ndsc-loaded/x_train.npy\")\n",
    "x_test = np.load(\"ndsc-loaded/x_test.npy\")\n",
    "y_train = np.load(\"ndsc-loaded/y_train.npy\")\n",
    "features = np.load(\"ndsc-loaded/features.npy\")\n",
    "test_features = np.load(\"ndsc-loaded/test_features.npy\")\n",
    "word_index = np.load(\"ndsc-loaded/word_index.npy\").item()\n",
    "embedding_matrix = np.load(\"ndsc-loaded/embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 15 18 ... 25 25  7]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(embedding_matrix[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78578eab64a477d0a5ad6b1c917ae154868a44df"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../input/ndsc-beginner/train.csv\")[:50000]\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92ffbf2ef35d2dc5863ee27c61eadd1869ca5440"
   },
   "outputs": [],
   "source": [
    "# # embdedding setup\n",
    "# # Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# embeddings_index = {}\n",
    "# f = open('../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "# for line in tqdm(f):\n",
    "#     values = line.split(\" \")\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# # print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cc8b0bcd285225ce651d024f506615d4656b9f7"
   },
   "outputs": [],
   "source": [
    "# # Convert values to embeddings\n",
    "# def text_to_array(text):\n",
    "#     empyt_emb = np.zeros(300)\n",
    "#     text = text[:-1].split()[:30]\n",
    "#     embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "#     embeds+= [empyt_emb] * (30 - len(embeds))\n",
    "#     return np.array(embeds)\n",
    "\n",
    "# # train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"title\"])]\n",
    "# val_vects = np.array([text_to_array(X_text) for X_text in (val_df[\"title\"][:4000])])\n",
    "# val_y = np.array(val_df[\"Category\"][:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         160,    5,  135,  126],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,  681,  501,   66,  765, 1046,\n",
       "         158,  226,  210,  213]], dtype=int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(x_train)-1)\n",
    "\n",
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124352 609812 515468 ... 583782 486744 101413]\n",
      "[[    0     0     0 ...    26    42   304]\n",
      " [    0     0     0 ...   176 12665  3459]\n",
      " [    0     0     0 ...   210   213   931]\n",
      " ...\n",
      " [    0     0     0 ...    24   799  1155]\n",
      " [    0     0     0 ...  1469   165    93]\n",
      " [    0     0     0 ...   375   402   594]]\n",
      "(1, 70, 300)\n",
      "[35 31 12 26 43 25 31 42  4  3 22 18 31  0  3 12  3  3 43  4  3 32 18 25\n",
      " 35 18  4 12 25 12 27 41  0  5  5 11 26 18 18 26  5 23 20  7 17  3  5 31\n",
      " 31 13  4 18 34  2  3  3 18 31 28  8 35 27 27 31  4 26  4 18  3 31 31 35\n",
      " 20 35  3 20 12  4  3  8  3 12  7  3  7 43 21  5 31 27 31  4  3 41 21  3\n",
      " 33 25 35 26 32 27 35  3 41  6  9  8  5 18 20 21 41  2 28  7 18 25  5  7\n",
      "  1 31 22 31  4 25  5 20]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(x_train)-1)\n",
    "np.random.shuffle(indices)\n",
    "i = 0\n",
    "x_train_batch_tokens = np.take(x_train, indices[i*batch_size:(i+1)*batch_size], axis = 0) \n",
    "x_train_batch = [[embedding_matrix[token] for token in title] for title in x_train_batch_tokens]\n",
    "y_train_batch = np.take(y_train, indices[i*batch_size:(i+1)*batch_size])\n",
    "print(indices)\n",
    "print(x_train_batch_tokens)\n",
    "print(np.shape(x_train_batch[:1]))\n",
    "print(y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "0c950448e0717eaebb920d93cfdc6b4561e21853"
   },
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 128\n",
    "\n",
    "indices = np.arange(len(x_train)-1)\n",
    "# print(len(x_train))\n",
    "# print(np.random.shuffle())\n",
    "\n",
    "def batch_gen():\n",
    "    n_batches = math.ceil(len(x_train) / batch_size)\n",
    "    while True: \n",
    "        np.random.shuffle(indices)  # Shuffle the data.\n",
    "#         print(indices)\n",
    "        for i in range(n_batches):\n",
    "#             texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "#             text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            \n",
    "            x_train_batch_tokens = np.take(x_train, indices[i*batch_size:(i+1)*batch_size], axis = 0) \n",
    "            x_train_batch = [[embedding_matrix[token] for token in title] for title in x_train_batch_tokens]\n",
    "            y_train_batch = np.take(y_train, indices[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            yield x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "706c0224b6112e5a8f00ad25f35e90fbb9519a5f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "798c303ec834fb530a60a1e590cfbd9a86f93fde"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "9ad418c95b31ab5691d49b72c7c9622ef9ea42cf"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n",
    "                        input_shape=(30, 300)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "model.add(Dense(58))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "3f0b703ded691293450beb4ddf2d903d0e08c737",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "[272711 209676 512525 ... 303719 144284 586224]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a2c8ab75bdd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(mg, epochs=1,\n\u001b[1;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     verbose=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mg = batch_gen()\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6cdda0e301be0b84e826e29f0f3a85c41aa6da9"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b58cd95254f41e5002a17de0c3feab54a5fc3c67"
   },
   "outputs": [],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"../input/ndsc-beginner/test.csv\")\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e6ed54def110c881f401c6f8a844752baedcfbd"
   },
   "outputs": [],
   "source": [
    "print(np.shape(all_preds)\n",
    "y_te = [np.argmax(pred) for pred in all_preds]\n",
    "\n",
    "submit_df = pd.DataFrame({\"itemid\": test_df[\"itemid\"], \"Category\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
